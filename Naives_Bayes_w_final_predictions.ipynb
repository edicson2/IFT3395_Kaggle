{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes naïf pour catégoriser les données. On suit les étapes décrites aux p.1-32 du document suivant :\n",
    "\n",
    "https://d1b10bmlvqabco.cloudfront.net/paste/k50wrw2ed8r5ry/540377b9394397bffc17bdea2943f0814617eeec4c78da972e18c64e9c03d6ad/Jurafsky_-_Naive_Bayes_-_Text_classification.pdf\n",
    "\n",
    "C'est de ce document qu'on parle dans le code quand on parle du \"document\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import string\n",
    "import math\n",
    "import matplotlib.pyplot as plt # juste pour afficher des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prétraitement des données : les 3 fonctions qui suivent prennent le dataset en transforment les données sous\n",
    "# 'Abstract' par les abstracts en un numpy.array où chaque élément est un string d'un mot de l'article.\n",
    "# Prend en considération les mots répétés plusieurs fois et enlève les mots communs qui ne veulent rien dire\n",
    "# comme \"it\", \"and\", \"we\", etc.\n",
    "\n",
    "# pour ponctuation\n",
    "def pd_translate(df):\n",
    "    df = df.replace(r'\\n',' ', regex=True)\n",
    "    punct = string.punctuation\n",
    "    transtab = str.maketrans(dict.fromkeys(punct, ' '))\n",
    "    return df.assign(\n",
    "        Abstract='¿'.join(df['Abstract'].tolist()).translate(transtab).split('¿')\n",
    "    )\n",
    "\n",
    "\n",
    "#pour enlever les mots communs\n",
    "def load_common_words(file):\n",
    "    common_words = pd.read_csv(file,header=None)\n",
    "    common_words = common_words[0].str.split(' ')\n",
    "    return np.array(common_words[0])\n",
    "\n",
    "# actual pré-traitement où on change le dataset pour de vrai\n",
    "def pre_traitement(df, common_words):\n",
    "    # Remove the punctuation, blank spaces and put everything in lowercase\n",
    "    df = df.assign(Abstract=pd_translate(df)['Abstract'].str.lower())\n",
    "    df = df.assign(Abstract=df['Abstract'].str.strip())\n",
    "    df = df.replace(r'\\s+',' ', regex=True)\n",
    "    \n",
    "    # Split each phrase in words\n",
    "    df = df.assign(Abstract=df['Abstract'].str.split(' '))\n",
    "\n",
    "    abstracts = []\n",
    "\n",
    "    for article in df['Abstract'] :\n",
    "        article = np.array(article)\n",
    "        index = []\n",
    "        for common_word in common_words :\n",
    "            index = np.where(article == common_word)\n",
    "            article = np.delete(article, index)\n",
    "        abstracts.append(article)\n",
    "\n",
    "    df = df.assign(Abstract=list(abstracts))\n",
    "\n",
    "    return df\n",
    "#### fin pré-traitement\n",
    "\n",
    "\n",
    "# dictionnaire de la fréquence de chaque mot pour chaque catégorie d'articles\n",
    "# e.g. on aura l'entrée {'astro-ph':{'energy:147'},...} si on l'applique au dataset de training\n",
    "# il faut lire que le mot \"energy\" apparaît 147 fois parmi tous les articles d'astrophysique en tenant\n",
    "# compte des répétitions (e.g. si un article dit 2 fois le mot energy, alors on ajoute 2 au compteur)\n",
    "def create_category_maps(data):\n",
    "    words_par_category = {}\n",
    "    # Create an entry for each unique category\n",
    "    for category in data['Category'].unique():\n",
    "        df_cat = data.loc[data['Category'] == category]\n",
    "        \n",
    "        total_class_words = np.array([])\n",
    "        \n",
    "        # Add the word to a list for the category\n",
    "        for row in df_cat['Abstract']:\n",
    "            total_class_words = np.concatenate((total_class_words, row), axis=None)\n",
    "        \n",
    "        # Count the number of times a word is repeated in a category\n",
    "        words_by_class = {}\n",
    "        for (i, elem) in enumerate(total_class_words):\n",
    "            if elem in words_by_class:\n",
    "                words_by_class[elem] += 1\n",
    "            else:\n",
    "                words_by_class[elem] = 1\n",
    "        \n",
    "        words_par_category[category] = words_by_class\n",
    "        \n",
    "    return words_par_category\n",
    "\n",
    "\n",
    "# nombre total de mots pour chaque catégorie (donc n de la p.32 du document)\n",
    "def n_words_classes(dataset) :\n",
    "    total_words_par_category = {}\n",
    "    #iterate over all categories\n",
    "    for category in dataset['Category'].unique() :\n",
    "        dataset_cat = data.loc[data['Category'] == category]\n",
    "        concatenated_abstracts = np.array([])\n",
    "        # append all abstracts together\n",
    "        for row in dataset_cat['Abstract'] :\n",
    "            concatenated_abstracts = np.concatenate((concatenated_abstracts, row), axis=None)\n",
    "        total_words_par_category[category] = concatenated_abstracts.size # total words for a particular class\n",
    "    return total_words_par_category\n",
    "\n",
    "\n",
    "# P(c), probability that a particular article is of a certain type when drawing a random article from dataset\n",
    "# i.e. P(c_j) p.32 du document = |docs_j| / |total # doc|\n",
    "def category_probability(dataset) :\n",
    "    n_articles_par_category = {}\n",
    "    n_articles_tot = len(dataset)\n",
    "    for category in dataset['Category'].unique() :\n",
    "        dataset_cat = data.loc[data['Category'] == category]\n",
    "        n_articles_par_category[category] = len(dataset_cat) / n_articles_tot\n",
    "    return n_articles_par_category\n",
    "\n",
    "\n",
    "# returns |V|=|Vocabulary| (p.32 of doc), a scalar, total unique words in ALL training articles\n",
    "def taille_V(dataset) :\n",
    "    concatenated_unique_words = np.array([])\n",
    "    for row in dataset['Abstract'] :\n",
    "        # append the unique words of each article\n",
    "        concatenated_unique_words = np.concatenate((concatenated_unique_words, np.unique(row)))\n",
    "    # return the unique words from these words\n",
    "    return(np.unique(concatenated_unique_words).size)\n",
    "\n",
    "# probability to select a particular word when selecting a word from a certain class type article\n",
    "# e.g. what is the probability of selecting the word \"solar\" when choosing a random word\n",
    "# from all astrophysics articles? see p.32 of document\n",
    "def prob_word_c(word, classe, V_size, alpha=1) :\n",
    "    if word in category_maps[classe] :\n",
    "        return (category_maps[classe][word] + alpha) / (total_words_par_class[classe] + alpha*V_size)\n",
    "    else :\n",
    "        return alpha / (total_words_par_class[classe] + alpha*V_size)\n",
    "\n",
    "# returns index of max likelihood subject for given test example \n",
    "def prob_article_c(test_article, classes, p_class, V_size, alpha=1) :\n",
    "    prob_word = np.zeros( (len(test_article), n_classes))\n",
    "    for (i, word) in enumerate(test_article) :\n",
    "        for (j, classe) in enumerate(unique_labels) :\n",
    "            prob_word[i,j] = p_class[classe]*np.log(prob_word_c(word, classe, V_size, alpha)) # * prob_category\n",
    "    return np.argmax(np.sum(prob_word, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# prétraitement des données : les 3 fonctions qui suivent prennent le dataset en transforment les données sous\n",
    "# 'Abstract' par les abstracts en un numpy.array où chaque élément est un string d'un mot de l'article.\n",
    "# Prend en considération les mots répétés plusieurs fois et enlève les mots communs qui ne veulent rien dire\n",
    "# comme \"it\", \"and\", \"we\", etc.\n",
    "\n",
    "# pour ponctuation\n",
    "def pd_translate(df):\n",
    "    df = df.replace(r'\\n',' ', regex=True)\n",
    "    punct = string.punctuation\n",
    "    transtab = str.maketrans(dict.fromkeys(punct, ' '))\n",
    "    return df.assign(\n",
    "        Abstract='¿'.join(df['Abstract'].tolist()).translate(transtab).split('¿')\n",
    "    )\n",
    "\n",
    "\n",
    "#pour enlever les mots communs\n",
    "def load_common_words(file):\n",
    "    common_words = pd.read_csv(file,header=None)\n",
    "    common_words = common_words[0].str.split(' ')\n",
    "    return np.array(common_words[0])\n",
    "\n",
    "# actual pré-traitement où on change le dataset pour de vrai\n",
    "def pre_traitement(df, common_words):\n",
    "    # Remove the punctuation, blank spaces and put everything in lowercase\n",
    "    df = df.assign(Abstract=pd_translate(df)['Abstract'].str.lower())\n",
    "    df = df.assign(Abstract=df['Abstract'].str.strip())\n",
    "    df = df.replace(r'\\s+',' ', regex=True)\n",
    "    \n",
    "    # Split each phrase in words\n",
    "    df = df.assign(Abstract=df['Abstract'].str.split(' '))\n",
    "\n",
    "    abstracts = []\n",
    "\n",
    "    for article in df['Abstract'] :\n",
    "        article = np.array(article)\n",
    "        index = []\n",
    "        for common_word in common_words :\n",
    "            index = np.where(article == common_word)\n",
    "            article = np.delete(article, index)\n",
    "        abstracts.append(article)\n",
    "\n",
    "    df = df.assign(Abstract=list(abstracts))\n",
    "\n",
    "    return df\n",
    "#### fin pré-traitement\n",
    "\n",
    "\n",
    "# dictionnaire de la fréquence de chaque mot pour chaque catégorie d'articles\n",
    "# e.g. on aura l'entrée {'astro-ph':{'energy:147'},...} si on l'applique au dataset de training\n",
    "# il faut lire que le mot \"energy\" apparaît 147 fois parmi tous les articles d'astrophysique en tenant\n",
    "# compte des répétitions (e.g. si un article dit 2 fois le mot energy, alors on ajoute 2 au compteur)\n",
    "def create_category_maps(data):\n",
    "    words_par_category = {}\n",
    "    # Create an entry for each unique category\n",
    "    for category in data['Category'].unique():\n",
    "        df_cat = data.loc[data['Category'] == category]\n",
    "        \n",
    "        total_class_words = np.array([])\n",
    "        \n",
    "        # Add the word to a list for the category\n",
    "        for row in df_cat['Abstract']:\n",
    "            total_class_words = np.concatenate((total_class_words, row), axis=None)\n",
    "        \n",
    "        # Count the number of times a word is repeated in a category\n",
    "        words_by_class = {}\n",
    "        for (i, elem) in enumerate(total_class_words):\n",
    "            if elem in words_by_class:\n",
    "                words_by_class[elem] += 1\n",
    "            else:\n",
    "                words_by_class[elem] = 1\n",
    "        \n",
    "        words_par_category[category] = words_by_class\n",
    "        \n",
    "    return words_par_category\n",
    "\n",
    "\n",
    "# nombre total de mots pour chaque catégorie (donc n de la p.32 du document)\n",
    "def n_words_classes(dataset) :\n",
    "    total_words_par_category = {}\n",
    "    #iterate over all categories\n",
    "    for category in dataset['Category'].unique() :\n",
    "        dataset_cat = data.loc[data['Category'] == category]\n",
    "        concatenated_abstracts = np.array([])\n",
    "        # append all abstracts together\n",
    "        for row in dataset_cat['Abstract'] :\n",
    "            concatenated_abstracts = np.concatenate((concatenated_abstracts, row), axis=None)\n",
    "        total_words_par_category[category] = concatenated_abstracts.size # total words for a particular class\n",
    "    return total_words_par_category\n",
    "\n",
    "\n",
    "# P(c), probability that a particular article is of a certain type when drawing a random article from dataset\n",
    "# i.e. P(c_j) p.32 du document = |docs_j| / |total # doc|\n",
    "def category_probability(dataset) :\n",
    "    n_articles_par_category = {}\n",
    "    n_articles_tot = len(dataset)\n",
    "    for category in dataset['Category'].unique() :\n",
    "        dataset_cat = data.loc[data['Category'] == category]\n",
    "        n_articles_par_category[category] = len(dataset_cat) / n_articles_tot\n",
    "    return n_articles_par_category\n",
    "\n",
    "\n",
    "# returns |V|=|Vocabulary| (p.32 of doc), a scalar, total unique words in ALL training articles\n",
    "def taille_V(dataset) :\n",
    "    concatenated_unique_words = np.array([])\n",
    "    for row in dataset['Abstract'] :\n",
    "        # append the unique words of each article\n",
    "        concatenated_unique_words = np.concatenate((concatenated_unique_words, np.unique(row)))\n",
    "    # return the unique words from these words\n",
    "    return(np.unique(concatenated_unique_words).size)\n",
    "\n",
    "# probability to select a particular word when selecting a word from a certain class type article\n",
    "# e.g. what is the probability of selecting the word \"solar\" when choosing a random word\n",
    "# from all astrophysics articles? see p.32 of document\n",
    "def prob_word_c(word, classe, V_size, alpha=1) :\n",
    "    if word in category_maps[classe] :\n",
    "        return (category_maps[classe][word] + alpha) / (total_words_par_class[classe] + alpha*V_size)\n",
    "    else :\n",
    "        return alpha / (total_words_par_class[classe] + alpha*V_size)\n",
    "\n",
    "# returns index of max likelihood subject for given test example \n",
    "def prob_article_c(test_article, classes, p_class, V_size, alpha=1) :\n",
    "    prob_word = np.zeros( (len(test_article), n_classes))\n",
    "    for (i, word) in enumerate(test_article) :\n",
    "        for (j, classe) in enumerate(unique_labels) :\n",
    "            prob_word[i,j] = p_class[classe]*np.log(prob_word_c(word, classe, V_size, alpha)) # * prob_category\n",
    "    return np.argmax(np.sum(prob_word, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables utiles plus tard\n",
    "df= pd.read_csv(\"train.csv\")                                  # données brutes (train)\n",
    "common_words = load_common_words('common_english_words.txt')  # mots communs à enlever\n",
    "unique_labels = df['Category'].unique()                       # liste des sujets\n",
    "n_classes = len(unique_labels)                                # nombre de classes \n",
    "data = pre_traitement(df, common_words)                       # données pré-traitées (train)\n",
    "category_maps = create_category_maps(data)                    # dictionnaire du nombre de fois que chaque mot apparaît pour chaque classe\n",
    "total_words_par_class = n_words_classes(data)                 # nombre total de mots d'un type d'article en particulier\n",
    "V_size = taille_V(data)                                       # |V| nombre de mots uniques dans TOUS les articles de training\n",
    "cat_prob = category_probability(df)                           # probabilité de chaque sujet \n",
    "\n",
    "df_test = pd.read_csv(\"test.csv\")                             # données brutes (test)\n",
    "data_test = pre_traitement(df_test, common_words)             # données pré-traitées (test)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On est maintenant prêts à faire les prédictions. Pour ne pas gaspiller de soumissions sur Gradescope, on sépare notre dataset en deux parties : données de test (80%) et données de validation (20%). On va entraîner puis tester les données labeled pour s'assurer que tout fonctionne bien. En plus, on fera un petit scan de l'hyperparamètre alpha pour choisir une valeur optimale de ce paramètre. C'est le seul hyperparamètre du problème*, donc on peut simplement choisir quelques valeurs arbitraires de alpha et choisir celle qui a le meilleur résultat dans les données de validation.\n",
    "\n",
    "\\* Il y a aussi l'hyperparamètre de quels mots on veut enlever (common words), mais on ne touche pas à ca, très compliqué d'en tirer des conclusions et on en parle un peu plus dans le rapport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# séparer les données en train+validation\n",
    "sep = int(4*len(data)/5)\n",
    "train_data = data.iloc[:sep]\n",
    "val_data = data.iloc[sep:]\n",
    "\n",
    "# créer les dictionnaires de mots, trouver n et |V| pour ces datasets\n",
    "category_maps = create_category_maps(train_data)\n",
    "total_words_par_class = n_words_classes(train_data)\n",
    "V_size = taille_V(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0. 10.  0. ...  5.  3.  5.]\n",
      " [ 0. 10.  8. ...  5.  3.  5.]\n",
      " [ 8. 10.  8. ...  5.  3.  5.]\n",
      " ...\n",
      " [ 8. 10.  8. ...  5.  3.  5.]\n",
      " [14. 10. 14. ...  5.  3.  5.]\n",
      " [14. 10. 14. ...  5.  3.  3.]]\n",
      "[['astro-ph' 'astro-ph.SR' 'astro-ph' ... 'hep-th' 'math.CO' 'hep-th']\n",
      " ['astro-ph' 'astro-ph.SR' 'astro-ph.CO' ... 'hep-th' 'math.CO' 'hep-th']\n",
      " ['astro-ph.CO' 'astro-ph.SR' 'astro-ph.CO' ... 'hep-th' 'math.CO'\n",
      "  'hep-th']\n",
      " ...\n",
      " ['astro-ph.CO' 'astro-ph.SR' 'astro-ph.CO' ... 'hep-th' 'math.CO'\n",
      "  'hep-th']\n",
      " ['astro-ph.GA' 'astro-ph.SR' 'astro-ph.GA' ... 'hep-th' 'math.CO'\n",
      "  'hep-th']\n",
      " ['astro-ph.GA' 'astro-ph.SR' 'astro-ph.GA' ... 'hep-th' 'math.CO'\n",
      "  'math.CO']]\n"
     ]
    }
   ],
   "source": [
    "# trouver les prédictions pour le validation set (attention aux indices car le validation set est un\n",
    "# sous-ensemble des données de training) en utilisant quelques alphas différents\n",
    "alpha_table = [0.01, 0.1, 0.5, 1, 1.5, 2, 5, 10, 20]\n",
    "n_alpha = len(alpha_table)\n",
    "n_val = len(val_data)\n",
    "predictions_val = np.ones((n_alpha, n_val))\n",
    "for j in range(n_alpha) :\n",
    "    for i in range(sep,len(data)) :\n",
    "        predictions_val[j, i-sep] = prob_article_c(val_data['Abstract'][i], unique_labels, cat_prob, V_size, alpha_table[j])\n",
    "print(predictions_val)\n",
    "predictions_val_string = unique_labels[np.array(predictions_val, dtype='int')] # prédictions human-friendly\n",
    "print(predictions_val_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False,  True, False, ...,  True,  True,  True],\n",
       "       [False,  True, False, ...,  True,  True,  True],\n",
       "       [ True,  True, False, ...,  True,  True,  True],\n",
       "       ...,\n",
       "       [ True,  True, False, ...,  True,  True,  True],\n",
       "       [False,  True,  True, ...,  True,  True,  True],\n",
       "       [False,  True,  True, ...,  True,  True, False]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comparer aux vrais labels du validation set\n",
    "predictions_val_string == np.array(val_data['Category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.77266667, 0.78933333, 0.786     , 0.78333333, 0.77933333,\n",
       "       0.77733333, 0.744     , 0.72066667])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculer le taux de réussite\n",
    "efficacite = np.sum(predictions_val_string == np.array(val_data['Category']), axis=1) / 1500\n",
    "efficacite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graphique pour le rapport : influence de alpha sur la performance du modèle\n",
    "# on utilise matplotlib (bibliothèque interdite pour le modèle) mais c'est juste pour afficher les résultats\n",
    "plt.figure('alpha')\n",
    "plt.scatter(alpha_table, efficacite)\n",
    "plt.xscale('log')\n",
    "plt.title('Influence de alpha sur la performance du modèle') # pas de $\\alpha$ car jupyter notebook est picky, mes excuses...\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('Score')\n",
    "plt.savefig('influence_alpha.png')\n",
    "#plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rendus ici, on sait que notre code n'a pas de bug grave et qu'il fonctionne assez bien. On a des taux de réussite semblables aux scores que l'on voit sur Kaggle des autres équipes! En plus, on voit que n'importe quel alpha entre 0.1 et 2 donne des scores très semblables et optimaux. Les dernières lignes de code ne sont pas vraiment nécessaires pour la correction du rapport mais elles montrent ce qui a été fait pour obtenir les prédictions finales pour la compétition Kaggle, utilisant tout le dataset de training pour entraîner et les données de test pour les prédictions. On utilise alpha=1 même si alpha=0.1 performe très légèrement mieux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6. 10. 10. ...  0.  9.  4.]\n"
     ]
    }
   ],
   "source": [
    "##### Les prédictions finales sur les données de test!\n",
    "n_test = len(data_test)\n",
    "predictions_test = np.ones(n_test)\n",
    "for i in range(n_test) :\n",
    "    predictions_test[i] = prob_article_c(data_test['Abstract'][i], unique_labels, cat_prob, V_size)\n",
    "print(predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['stat.ML', 'astro-ph.SR', 'astro-ph.SR', ..., 'astro-ph', 'gr-qc',\n",
       "       'cond-mat.mes-hall'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# human-friendly\n",
    "predictions_string = unique_labels[np.array(predictions_test, dtype='int')]\n",
    "predictions_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Id': array([    0,     1,     2, ..., 14997, 14998, 14999]),\n",
       " 'Category': array(['stat.ML', 'astro-ph.SR', 'astro-ph.SR', ..., 'astro-ph', 'gr-qc',\n",
       "        'cond-mat.mes-hall'], dtype=object)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# créer un fichier .csv tel que requis\n",
    "predictions_dict = {'Id' : np.arange(len(predictions_string)), 'Category' : predictions_string}\n",
    "predictions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>stat.ML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>astro-ph.SR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>astro-ph.SR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>math.AP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>cs.LG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14995</td>\n",
       "      <td>14995</td>\n",
       "      <td>astro-ph.CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14996</td>\n",
       "      <td>14996</td>\n",
       "      <td>physics.optics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14997</td>\n",
       "      <td>14997</td>\n",
       "      <td>astro-ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14998</td>\n",
       "      <td>14998</td>\n",
       "      <td>gr-qc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14999</td>\n",
       "      <td>14999</td>\n",
       "      <td>cond-mat.mes-hall</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id           Category\n",
       "0          0            stat.ML\n",
       "1          1        astro-ph.SR\n",
       "2          2        astro-ph.SR\n",
       "3          3            math.AP\n",
       "4          4              cs.LG\n",
       "...      ...                ...\n",
       "14995  14995        astro-ph.CO\n",
       "14996  14996     physics.optics\n",
       "14997  14997           astro-ph\n",
       "14998  14998              gr-qc\n",
       "14999  14999  cond-mat.mes-hall\n",
       "\n",
       "[15000 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vérifications et visualisation du dataframe\n",
    "predictions_test_dataframe = pd.DataFrame(predictions_dict, columns = ['Id', 'Category'])\n",
    "predictions_test_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>stat.ML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>astro-ph.SR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>astro-ph.SR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>math.AP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>cs.LG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14995</td>\n",
       "      <td>14995</td>\n",
       "      <td>astro-ph.CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14996</td>\n",
       "      <td>14996</td>\n",
       "      <td>physics.optics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14997</td>\n",
       "      <td>14997</td>\n",
       "      <td>astro-ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14998</td>\n",
       "      <td>14998</td>\n",
       "      <td>gr-qc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14999</td>\n",
       "      <td>14999</td>\n",
       "      <td>cond-mat.mes-hall</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id           Category\n",
       "0          0            stat.ML\n",
       "1          1        astro-ph.SR\n",
       "2          2        astro-ph.SR\n",
       "3          3            math.AP\n",
       "4          4              cs.LG\n",
       "...      ...                ...\n",
       "14995  14995        astro-ph.CO\n",
       "14996  14996     physics.optics\n",
       "14997  14997           astro-ph\n",
       "14998  14998              gr-qc\n",
       "14999  14999  cond-mat.mes-hall\n",
       "\n",
       "[15000 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exportation sous csv et lecture du doc pour s'assurer qu'il est du bon format pour pas gaspiller une évaluation\n",
    "# Kaggle sur des niaiseries de tailles incompatibles\n",
    "predictions_test_dataframe.to_csv('predictions_bayes_naif_test_set.csv', index=False)\n",
    "dti = pd.read_csv('predictions_bayes_naif_test_set.csv')\n",
    "dti"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
